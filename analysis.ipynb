{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# Sorter per kandidat\n",
    "# Inkluder timestamp for hver post\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "import re\n",
    "import pickle\n",
    "from operator import itemgetter\n",
    "\n",
    "# Alpha-num, lowercase\n",
    "def easy_clean(comment):\n",
    "    comment = re.sub('[^A-Za-z ]+', '', comment['body']).lower().split(' ')\n",
    "    #comment = [re.sub('[^A-Za-z]+', '', word) for word in comment]\n",
    "    return comment\n",
    "\n",
    "# nltk tokenize, stopwords\n",
    "def nltk_clean(comment):\n",
    "    #comment = comment\n",
    "    #comment = nltk.sent_tokenize(comment)\n",
    "    print(comment)\n",
    "    comment = nltk.tokenize.word_tokenize(comment)\n",
    "    comment = [word for word in comment if word.isalpha()]\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    comment = [word for word in comment if word not in stop_words]\n",
    "    return comment\n",
    "\n",
    "# Remove quotes, clean text using translation table (punctuations)\n",
    "def nltk2_clean(comment): #tokenize, removes case, remove special characters and numbers\n",
    "    comment = [e for e in comment.splitlines() if e[:2] != '&g']\n",
    "    comment = ' '.join(comment)\n",
    "    comment = comment.split(' ')\n",
    "    comment = [w.lower() for w in comment]\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in comment]\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Tokenize at sentence level, applies nltk2_clean on each sentence\n",
    "def clean_sent(comment): #tokenize, removes case, remove special characters and numbers\n",
    "    comment = comment['body']\n",
    "    comments = nltk.sent_tokenize(comment)\n",
    "    comments = [nltk2_clean(sent) for sent in comments]\n",
    "    return comments\n",
    "\n",
    "\n",
    "# SÃ†TNINGSDELING REGEX: (?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\n",
    "\n",
    "#    sentence_list = tokenize.sent_tokenize(paragraph)\n",
    "#    paragraphSentiments = 0.0\n",
    "#    for sentence in sentence_list:\n",
    "#        vs = analyzer.polarity_scores(sentence)\n",
    "#        print(\"{:-<69} {}\".format(sentence, str(vs[\"compound\"])))\n",
    "#        paragraphSentiments += vs[\"compound\"]\n",
    "#    print(\"AVERAGE SENTIMENT FOR PARAGRAPH: \\t\" + str(round(paragraphSentiments / len(sentence_list), 4)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starter\n",
      "clinton_politics_2016-01-01_2016-02-01.json\n",
      "done\n",
      "\n",
      "clinton_politics_2016-02-01_2016-03-01.json\n",
      "done\n",
      "\n",
      "clinton_politics_2016-03-01_2016-04-01.json\n",
      "done\n",
      "\n",
      "clinton_politics_2016-04-01_2016-05-01.json\n",
      "done\n",
      "\n",
      "clinton_politics_2016-05-01_2016-06-01.json\n",
      "done\n",
      "\n",
      "clinton_politics_2016-06-01_2016-07-01.json\n",
      "done\n",
      "\n",
      "clinton_politics_2016-07-01_2016-08-01.json\n",
      "done\n",
      "\n",
      "clinton_politics_2016-08-01_2016-09-01.json\n",
      "done\n",
      "\n",
      "clinton_politics_2016-09-01_2016-10-01.json\n",
      "done\n",
      "\n",
      "clinton_politics_2016-10-01_2016-11-01.json\n",
      "done\n",
      "\n",
      "clinton_politics_2016-11-01_2016-12-01.json\n",
      "done\n",
      "\n",
      "clinton_politics_2016-12-01_2016-12-31.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-01-01_2016-02-01.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-02-01_2016-03-01.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-03-01_2016-04-01.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-04-01_2016-05-01.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-05-01_2016-06-01.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-06-01_2016-07-01.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-07-01_2016-08-01.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-08-01_2016-09-01.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-09-01_2016-10-01.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-10-01_2016-11-01.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-11-01_2016-12-01.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-12-01_2016-12-31.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-01-01_2016-02-01.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-02-01_2016-03-01.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-03-01_2016-04-01.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-04-01_2016-05-01.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-05-01_2016-06-01.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-06-01_2016-07-01.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-07-01_2016-08-01.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-08-01_2016-09-01.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-09-01_2016-10-01.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-10-01_2016-11-01.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-11-01_2016-12-01.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-12-01_2016-12-31.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-01-01_2016-02-01.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-02-01_2016-03-01.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-03-01_2016-04-01.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-04-01_2016-05-01.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-05-01_2016-06-01.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-06-01_2016-07-01.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-07-01_2016-08-01.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-08-01_2016-09-01.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-09-01_2016-10-01.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-10-01_2016-11-01.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-11-01_2016-12-01.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-12-01_2016-12-31.json\n",
      "done\n",
      "\n",
      "...Done in 203.40216255187988\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates across dataSet\n",
    "clSet = [] \n",
    "crSet = []\n",
    "trSet = []\n",
    "saSet = []\n",
    "\n",
    "print('starter')\n",
    "dataSet = []\n",
    "start = time.time()\n",
    "n = 0\n",
    "for file in os.listdir('data/'):\n",
    "    with open('data/'+file) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if file[:2] == 'cl':\n",
    "        candidates =  ['bernie' ,'sanders' ,'donald' ,'trump' ,'ted' ,'cruz']\n",
    "        #tempData = [e for e in data if not any(item in candidates for item in easy_clean(e))]\n",
    "        clData = [e for e in data if not any(item in candidates for item in easy_clean(e))]\n",
    "        clSet = clSet + clData\n",
    "        \n",
    "    elif file[:2] == 'cr':\n",
    "        candidates =  ['bernie' ,'sanders' ,'donald' ,'trump', 'hillary', 'clinton']\n",
    "        #tempData = [e for e in data if not any(item in candidates for item in easy_clean(e))]\n",
    "        crData = [e for e in data if not any(item in candidates for item in easy_clean(e))]\n",
    "        crSet = crSet + crData\n",
    "\n",
    "    elif file[:2] == 'tr':\n",
    "        candidates =  ['bernie' ,'sanders' ,'ted' ,'cruz', 'hillary', 'clinton']\n",
    "        #tempData = [e for e in data if not any(item in candidates for item in easy_clean(e))]\n",
    "        trData = [e for e in data if not any(item in candidates for item in easy_clean(e))]\n",
    "        trSet = trSet + trData\n",
    "        \n",
    "    elif file[:2] == 'sa':\n",
    "        candidates =  ['hillary', 'clinton' ,'donald' ,'trump' ,'ted' ,'cruz']\n",
    "        #tempData = [e for e in data if not any(item in candidates for item in easy_clean(e))]\n",
    "        saData = [e for e in data if not any(item in candidates for item in easy_clean(e))]\n",
    "        saSet = saSet+saData\n",
    "    print(file + '\\ndone\\n')\n",
    "    \n",
    "end = time.time()\n",
    "print('...Done in '+str(end - start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "dataSet = [clSet, crSet, trSet, saSet]\n",
    "dataSet = [[[clean_sent(comment),comment['created_utc']] for comment in candidate] for candidate in dataSet]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "with open(\"nodupData\", \"wb\") as fp:\n",
    "    pickle.dump(dataSet, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-d243b4005a7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Calculate polarity score of each sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m sentScores = [[[[sentence, analyzer.polarity_scores(sentence)['compound']]\n\u001b[0;32m----> 9\u001b[0;31m              for sentence in comment if len(sentence)>1] for comment in cleanData] for cleanData in dataSet]\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Calculate average polarity score for each sentence in comment + comment in string format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-135-d243b4005a7e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Calculate polarity score of each sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m sentScores = [[[[sentence, analyzer.polarity_scores(sentence)['compound']]\n\u001b[0;32m----> 9\u001b[0;31m              for sentence in comment if len(sentence)>1] for comment in cleanData] for cleanData in dataSet]\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Calculate average polarity score for each sentence in comment + comment in string format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-135-d243b4005a7e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Calculate polarity score of each sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m sentScores = [[[[sentence, analyzer.polarity_scores(sentence)['compound']]\n\u001b[0;32m----> 9\u001b[0;31m              for sentence in comment if len(sentence)>1] for comment in cleanData] for cleanData in dataSet]\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Calculate average polarity score for each sentence in comment + comment in string format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-135-d243b4005a7e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Calculate polarity score of each sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m sentScores = [[[[sentence, analyzer.polarity_scores(sentence)['compound']]\n\u001b[0;32m----> 9\u001b[0;31m              for sentence in comment if len(sentence)>1] for comment in cleanData] for cleanData in dataSet]\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Calculate average polarity score for each sentence in comment + comment in string format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/nltk/sentiment/vader.py\u001b[0m in \u001b[0;36mpolarity_scores\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mvalence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \"\"\"\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0msentitext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentiText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;31m# text, words_and_emoticons, is_cap_diff = self.preprocess(text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/nltk/sentiment/vader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords_and_emoticons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_words_and_emoticons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;31m# doesn't separate words from\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;31m# adjacent punctuation (keeps emoticons & contractions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/nltk/sentiment/vader.py\u001b[0m in \u001b[0;36m_words_and_emoticons\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mwes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0mwords_punc_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_words_plus_punc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0mwes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwe\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwes\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/nltk/sentiment/vader.py\u001b[0m in \u001b[0;36m_words_plus_punc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mwords_only\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_only\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;31m# the product gives ('cat', ',') and (',', 'cat')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0mpunc_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPUNC_LIST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0mpunc_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPUNC_LIST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mwords_punc_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpunc_before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/nltk/sentiment/vader.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mwords_only\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_only\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;31m# the product gives ('cat', ',') and (',', 'cat')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0mpunc_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPUNC_LIST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0mpunc_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPUNC_LIST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mwords_punc_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpunc_before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "sentences = ['that food was awful','what a beautiful dress','lock her up!','she is a shill','thomas is not very bright']\n",
    "analyzer = SIA()\n",
    "sent = [[' '.join(sentence), analyzer.polarity_scores(sentence[0])] for sentence in cleanData]\n",
    "\n",
    "# Calculate polarity score of each sentence\n",
    "sentScores = [[[[sentence, analyzer.polarity_scores(sentence)['compound']]\n",
    "             for sentence in comment if len(sentence)>1] for comment in cleanData] for cleanData in dataSet]\n",
    "\n",
    "# Calculate average polarity score for each sentence in comment + comment in string format\n",
    "commentScores = []\n",
    "for candidate in sentScores:\n",
    "    candidateScores = []\n",
    "    for comment in candidate:\n",
    "        sentence = '. '.join([sentence[0] for sentence in comment])\n",
    "        avg = np.mean([sentence[1] for sentence in comment])\n",
    "        commentScores.append([sentence, avg])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by compound\n",
    "\n",
    "commentScores.sort(key=lambda e: e[1],reverse=True)\n",
    "\n",
    "i = -9\n",
    "print(commentScores[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
