{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# - lav regex/funktion til at dele korrekt ved sætninger\n",
    "# - kør vader på hver sætning og returner gennemsnitlig compound\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# Alpha-num, lowercase\n",
    "def easy_clean(comment):\n",
    "    comment = re.sub('[^A-Za-z ]+', '', comment['body']).lower().split(' ')\n",
    "    #comment = [re.sub('[^A-Za-z]+', '', word) for word in comment]\n",
    "    return comment\n",
    "\n",
    "# nltk tokenize, stopwords\n",
    "def nltk_clean(comment):\n",
    "    #comment = comment\n",
    "    #comment = nltk.sent_tokenize(comment)\n",
    "    print(comment)\n",
    "    comment = nltk.tokenize.word_tokenize(comment)\n",
    "    comment = [word for word in comment if word.isalpha()]\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    comment = [word for word in comment if word not in stop_words]\n",
    "    return comment\n",
    "\n",
    "# Remove quotes, clean text using translation table (punctuations)\n",
    "def nltk2_clean(comment): #tokenize, removes case, remove special characters and numbers\n",
    "    comment = [e for e in comment.splitlines() if e[:2] != '&g']\n",
    "    comment = ' '.join(comment)\n",
    "    comment = comment.split(' ')\n",
    "    comment = [w.lower() for w in comment]\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in comment]\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Tokenize at sentence level, applies nltk2_clean on each sentence\n",
    "def clean_sent(comment): #tokenize, removes case, remove special characters and numbers\n",
    "    comment = comment['body']\n",
    "    comments = nltk.sent_tokenize(comment)\n",
    "    comments = [nltk2_clean(sent) for sent in comments]\n",
    "    return comments\n",
    "\n",
    "\n",
    "# SÆTNINGSDELING REGEX: (?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\n",
    "\n",
    "#    sentence_list = tokenize.sent_tokenize(paragraph)\n",
    "#    paragraphSentiments = 0.0\n",
    "#    for sentence in sentence_list:\n",
    "#        vs = analyzer.polarity_scores(sentence)\n",
    "#        print(\"{:-<69} {}\".format(sentence, str(vs[\"compound\"])))\n",
    "#        paragraphSentiments += vs[\"compound\"]\n",
    "#    print(\"AVERAGE SENTIMENT FOR PARAGRAPH: \\t\" + str(round(paragraphSentiments / len(sentence_list), 4)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starter\n",
      "clinton_politics_2016-01-01_2016-02-01.json\n",
      "done\n",
      "\n",
      "clinton_politics_2016-02-01_2016-03-01.json\n",
      "done\n",
      "\n",
      "clinton_politics_2016-03-01_2016-04-01.json\n",
      "done\n",
      "\n",
      "clinton_politics_2016-04-01_2016-05-01.json\n",
      "done\n",
      "\n",
      "clinton_politics_2016-05-01_2016-06-01.json\n",
      "done\n",
      "\n",
      "clinton_politics_2016-06-01_2016-07-01.json\n",
      "done\n",
      "\n",
      "clinton_politics_2016-07-01_2016-08-01.json\n",
      "done\n",
      "\n",
      "clinton_politics_2016-08-01_2016-09-01.json\n",
      "done\n",
      "\n",
      "clinton_politics_2016-09-01_2016-10-01.json\n",
      "done\n",
      "\n",
      "clinton_politics_2016-10-01_2016-11-01.json\n",
      "done\n",
      "\n",
      "clinton_politics_2016-11-01_2016-12-01.json\n",
      "done\n",
      "\n",
      "clinton_politics_2016-12-01_2016-12-31.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-01-01_2016-02-01.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-02-01_2016-03-01.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-03-01_2016-04-01.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-04-01_2016-05-01.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-05-01_2016-06-01.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-06-01_2016-07-01.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-07-01_2016-08-01.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-08-01_2016-09-01.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-09-01_2016-10-01.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-10-01_2016-11-01.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-11-01_2016-12-01.json\n",
      "done\n",
      "\n",
      "cruz_politics_2016-12-01_2016-12-31.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-01-01_2016-02-01.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-02-01_2016-03-01.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-03-01_2016-04-01.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-04-01_2016-05-01.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-05-01_2016-06-01.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-06-01_2016-07-01.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-07-01_2016-08-01.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-08-01_2016-09-01.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-09-01_2016-10-01.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-10-01_2016-11-01.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-11-01_2016-12-01.json\n",
      "done\n",
      "\n",
      "sanders_politics_2016-12-01_2016-12-31.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-01-01_2016-02-01.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-02-01_2016-03-01.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-03-01_2016-04-01.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-04-01_2016-05-01.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-05-01_2016-06-01.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-06-01_2016-07-01.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-07-01_2016-08-01.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-08-01_2016-09-01.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-09-01_2016-10-01.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-10-01_2016-11-01.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-11-01_2016-12-01.json\n",
      "done\n",
      "\n",
      "trump_politics_2016-12-01_2016-12-31.json\n",
      "done\n",
      "\n",
      "...Done in 181.36869478225708\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates across dataSet\n",
    "\n",
    "clSet = [] \n",
    "crSet = []\n",
    "trSet = []\n",
    "saSet = []\n",
    "\n",
    "\n",
    "print('starter')\n",
    "dataSet = []\n",
    "start = time.time()\n",
    "n = 0\n",
    "for file in os.listdir('data/'):\n",
    "    with open('data/'+file) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if file[:2] == 'cl':\n",
    "        candidates =  ['bernie' ,'sanders' ,'donald' ,'trump' ,'ted' ,'cruz']\n",
    "        #tempData = [e for e in data if not any(item in candidates for item in easy_clean(e))]\n",
    "        clData = [e for e in data if not any(item in candidates for item in easy_clean(e))]\n",
    "        clSet = clSet + clData\n",
    "        \n",
    "    elif file[:2] == 'cr':\n",
    "        candidates =  ['bernie' ,'sanders' ,'donald' ,'trump', 'hillary', 'clinton']\n",
    "        #tempData = [e for e in data if not any(item in candidates for item in easy_clean(e))]\n",
    "        crData = [e for e in data if not any(item in candidates for item in easy_clean(e))]\n",
    "        crSet = crSet + crData\n",
    "\n",
    "    elif file[:2] == 'tr':\n",
    "        candidates =  ['bernie' ,'sanders' ,'ted' ,'cruz', 'hillary', 'clinton']\n",
    "        #tempData = [e for e in data if not any(item in candidates for item in easy_clean(e))]\n",
    "        trData = [e for e in data if not any(item in candidates for item in easy_clean(e))]\n",
    "        trSet = trSet + trData\n",
    "        \n",
    "    elif file[:2] == 'sa':\n",
    "        candidates =  ['hillary', 'clinton' ,'donald' ,'trump' ,'ted' ,'cruz']\n",
    "        #tempData = [e for e in data if not any(item in candidates for item in easy_clean(e))]\n",
    "        saData = [e for e in data if not any(item in candidates for item in easy_clean(e))]\n",
    "        saSet = saSet+saData\n",
    "    print(file + '\\ndone\\n')\n",
    "    \n",
    "end = time.time()\n",
    "print('...Done in '+str(end - start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "\n",
    "dataSet = [clSet, crSet, trSet, saSet]\n",
    "with open(\"nodupData\", \"wb\") as fp:\n",
    "    pickle.dump(dataSet, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9600\n",
      "['you are twisting them by equating regular staffers versus astroturfing. so are you saying its ethical for a presidential candidate to try and manipulate public perception through anonymous boards. it doesnt matter if astrotufing gains hillary vote or votes. its not ethical to employ individuals to act as regular people with no direct affiliation', 0.058724999999999999]\n",
      "\n",
      "\n",
      "[['you are twisting them by equating regular staffers versus astroturfing', 0.0], ['so are you saying its ethical for a presidential candidate to try and manipulate public perception through anonymous boards', 0.5106], ['it doesnt matter if astrotufing gains hillary vote or votes', 0.3239], ['its not ethical to employ individuals to act as regular people with no direct affiliation', -0.5996]]\n"
     ]
    }
   ],
   "source": [
    "# Import vader sentiment analyser\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "sentences = ['that food was awful','what a beautiful dress','lock her up!','she is a shill','thomas is not very bright']\n",
    "analyzer = SIA()\n",
    "sent = [[' '.join(sentence), analyzer.polarity_scores(sentence[0])] for sentence in cleanData]\n",
    "\n",
    "# Calculate polarity score of each sentence\n",
    "sentScores = [[[sentence, analyzer.polarity_scores(sentence)['compound']]\n",
    "             for sentence in comment if len(sentence)>1] for comment in cleanData]\n",
    "\n",
    "# Calculate average polarity score for each sentence in comment + comment in string format\n",
    "commentScores = []\n",
    "for comment in sentScores:\n",
    "    sentence = '. '.join([sentence[0] for sentence in comment])\n",
    "    avg = np.mean([sentence[1] for sentence in comment])\n",
    "    commentScores.append([sentence, avg])\n",
    "\n",
    "print(len(commentScores))    \n",
    "print(commentScores[0])\n",
    "print('\\n')\n",
    "print(sentScores[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if trump wins hell probably end up trying to abuse his position and get snl taken off the air considering his repeatedly expressed desire to expand libel laws in order to attack anyone who criticizes him', -0.76500000000000001]\n"
     ]
    }
   ],
   "source": [
    "# Sort by compound\n",
    "from operator import itemgetter\n",
    "\n",
    "commentScores.sort(key=lambda e: e[1],reverse=True)\n",
    "\n",
    "i = -9\n",
    "print(commentScores[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
